#!/usr/bin/env python3
"""
Minimal test of the agents.py workflow to isolate issues
"""

import asyncio
import sys
from pathlib import Path

# Add the parent directory to import agents
sys.path.append(str(Path(__file__).parent.parent))

async def test_minimal_agent():
    """Test the agent workflow with minimal setup"""
    
    print("ğŸ§ª Testing minimal agent workflow...")
    print("=" * 50)
    
    try:
        # Import agents
        from agents import process_building_automation_request
        
        print("âœ… Successfully imported agents module")
        
        # Test query
        test_query = "Zone 2_3 has CO2 at 1350ppm. What should I do immediately?"
        print(f"ğŸ“ Test query: {test_query}")
        
        print("\nğŸ”„ Calling agent with baseline model...")
        
        # Add timeout to prevent hanging
        result = await asyncio.wait_for(
            process_building_automation_request(test_query, model_type="baseline"),
            timeout=300.0  # 30 second timeout
        )
        
        print("\nâœ… Agent completed successfully!")
        print(f"ğŸ“¤ AI Response: {result.get('ai_response', 'No response')[:200]}...")
        print(f"ğŸ“Š Analysis Type: {result.get('analysis_type', 'Unknown')}")
        print(f"â±ï¸  Status: {result.get('status', 'Unknown')}")
        
        return True
        
    except asyncio.TimeoutError:
        print("\nâ° TIMEOUT: Agent took longer than 30 seconds")
        print("ğŸ’¡ This suggests the building API is slow or hanging")
        return False
        
    except Exception as e:
        print(f"\nâŒ ERROR: {str(e)}")
        print(f"ğŸ“ Error type: {type(e).__name__}")
        return False

async def test_ollama_directly():
    """Test if Ollama is responding quickly"""
    
    print("\nğŸ¦™ Testing Ollama directly...")
    print("=" * 30)
    
    try:
        from langchain_ollama import ChatOllama
        
        llm = ChatOllama(
            model="qwen3:4b-q8_0",
            temperature=0.1,
        )
        
        # Simple test without tools
        response = await asyncio.wait_for(
            llm.ainvoke("What is 2+2?"),
            timeout=300.0
        )
        
        print("âœ… Ollama responding correctly")
        print(f"ğŸ“¤ Response: {response.content[:100]}...")
        return True
        
    except asyncio.TimeoutError:
        print("â° Ollama is slow or hanging")
        return False
        
    except Exception as e:
        print(f"âŒ Ollama error: {str(e)}")
        return False

async def main():
    """Run all tests"""
    
    print("ğŸš€ DEBUGGING LANGGRAPH WORKFLOW")
    print("=" * 60)
    
    # Test 1: Ollama
    ollama_ok = await test_ollama_directly()
    
    # Test 2: Full agent
    agent_ok = await test_minimal_agent()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ TEST SUMMARY:")
    print(f"ğŸ¦™ Ollama: {'âœ… PASS' if ollama_ok else 'âŒ FAIL'}")
    print(f"ğŸ¤– Agent: {'âœ… PASS' if agent_ok else 'âŒ FAIL'}")
    
    if ollama_ok and agent_ok:
        print("\nğŸ‰ Everything working! The issue might be with Promptfoo integration.")
    elif ollama_ok and not agent_ok:
        print("\nâš ï¸  Ollama works but agent fails. Check your building API.")
    else:
        print("\nâŒ Basic Ollama failing. Check your Ollama installation.")

if __name__ == "__main__":
    asyncio.run(main())